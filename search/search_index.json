{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeoAI-Tutorials","text":"<p>A collection of Jupyter notebook examples for using GeoAI</p>"},{"location":"#population-dynamics-foundation-model","title":"Population Dynamics Foundation Model","text":"<p>Tutorials for Using Google's Population Dynamics Foundation Model (PDFM)</p> <ul> <li>Predicting US Housing Prices with PDFM and Zillow Data</li> <li>Visualizing PDFM Features and Predicted Home Values</li> <li>PDFM super resolution with Zillow housing data</li> </ul>"},{"location":"#demos","title":"Demos","text":"<ul> <li>Visualize US meidan home values at the county level</li> </ul> <ul> <li>Visualize PDFM features</li> </ul> <ul> <li>PDFM super resolution</li> </ul>"},{"location":"PDFM/map_pdfm_features/","title":"Map pdfm features","text":"<p>Mapping PDFM Features and Predicted Housing Prices</p> In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[maplibre]\" scikit-learn\n</pre> # %pip install \"leafmap[maplibre]\" scikit-learn In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\nimport leafmap.maplibregl as leafmap\n</pre> import os import pandas as pd import geopandas as gpd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor from leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file import leafmap.maplibregl as leafmap In\u00a0[\u00a0]: Copied! <pre>zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\"\nzhvi_file = \"data/zillow_home_value_index_by_county.csv\"\n</pre> zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\" zhvi_file = \"data/zillow_home_value_index_by_county.csv\" In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(zhvi_file):\n    download_file(zhvi_url, zhvi_file)\n</pre> if not os.path.exists(zhvi_file):     download_file(zhvi_url, zhvi_file) In\u00a0[\u00a0]: Copied! <pre>zhvi_df = pd.read_csv(\n    zhvi_file, dtype={\"StateCodeFIPS\": \"string\", \"MunicipalCodeFIPS\": \"string\"}\n)\nzhvi_df.index = \"geoId/\" + zhvi_df[\"StateCodeFIPS\"] + zhvi_df[\"MunicipalCodeFIPS\"]\nzhvi_df.head()\n</pre> zhvi_df = pd.read_csv(     zhvi_file, dtype={\"StateCodeFIPS\": \"string\", \"MunicipalCodeFIPS\": \"string\"} ) zhvi_df.index = \"geoId/\" + zhvi_df[\"StateCodeFIPS\"] + zhvi_df[\"MunicipalCodeFIPS\"] zhvi_df.head() In\u00a0[\u00a0]: Copied! <pre>county_geojson = \"data/county.geojson\"\nif not os.path.exists(county_geojson):\n    raise FileNotFoundError(\"Please request the embeddings from Google\")\n</pre> county_geojson = \"data/county.geojson\" if not os.path.exists(county_geojson):     raise FileNotFoundError(\"Please request the embeddings from Google\") In\u00a0[\u00a0]: Copied! <pre>county_gdf = gpd.read_file(county_geojson)\ncounty_gdf.set_index(\"place\", inplace=True)\ncounty_gdf.head()\n</pre> county_gdf = gpd.read_file(county_geojson) county_gdf.set_index(\"place\", inplace=True) county_gdf.head() In\u00a0[\u00a0]: Copied! <pre>df = zhvi_df.join(county_gdf)\nzhvi_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nzhvi_gdf.head()\n</pre> df = zhvi_df.join(county_gdf) zhvi_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\") zhvi_gdf.head() In\u00a0[\u00a0]: Copied! <pre>column = \"2024-10-31\"\ngdf = zhvi_gdf[[\"RegionName\", \"State\", column, \"geometry\"]]\ngdf.head()\n</pre> column = \"2024-10-31\" gdf = zhvi_gdf[[\"RegionName\", \"State\", column, \"geometry\"]] gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\")\nfirst_symbol_id = m.find_first_symbol_layer()[\"id\"]\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=\"Median Home Value\",\n    name=\"Median Home Value\",\n    before_id=first_symbol_id,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\") first_symbol_id = m.find_first_symbol_layer()[\"id\"] m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=\"Median Home Value\",     name=\"Median Home Value\",     before_id=first_symbol_id, ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=\"Median Home Value\",\n    extrude=True,\n    scale_factor=3,\n    before_id=first_symbol_id,\n    name=\"Median Home Value\",\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=\"Median Home Value\",     extrude=True,     scale_factor=3,     before_id=first_symbol_id,     name=\"Median Home Value\", ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>embeddings_file_path = \"data/county_embeddings.csv\"\n</pre> embeddings_file_path = \"data/county_embeddings.csv\" In\u00a0[\u00a0]: Copied! <pre>embeddings_df = pd.read_csv(embeddings_file_path).set_index(\"place\")\nembeddings_df.head()\n</pre> embeddings_df = pd.read_csv(embeddings_file_path).set_index(\"place\") embeddings_df.head() In\u00a0[\u00a0]: Copied! <pre>df = embeddings_df.join(county_gdf)\nembeddings_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nembeddings_gdf.head()\n</pre> df = embeddings_df.join(county_gdf) embeddings_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\") embeddings_gdf.head() In\u00a0[\u00a0]: Copied! <pre>column = \"feature329\"  # Change this to the feature you want to use\ngdf = embeddings_gdf[[column, \"state\", \"county\", \"geometry\"]]\ngdf.head()\n</pre> column = \"feature329\"  # Change this to the feature you want to use gdf = embeddings_gdf[[column, \"state\", \"county\", \"geometry\"]] gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=column,\n    before_id=first_symbol_id,\n    name=column,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\") m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=column,     before_id=first_symbol_id,     name=column, ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=column,\n    before_id=first_symbol_id,\n    name=column,\n    extrude=True,\n    scale_factor=0.00005,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=column,     before_id=first_symbol_id,     name=column,     extrude=True,     scale_factor=0.00005, ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>data = zhvi_df.join(embeddings_df, how=\"inner\")\ndata.head()\n</pre> data = zhvi_df.join(embeddings_df, how=\"inner\") data.head() In\u00a0[\u00a0]: Copied! <pre>embedding_features = [f\"feature{x}\" for x in range(330)]\nlabel = \"2024-10-31\"  # Change this to the date you want to predict\n</pre> embedding_features = [f\"feature{x}\" for x in range(330)] label = \"2024-10-31\"  # Change this to the date you want to predict In\u00a0[\u00a0]: Copied! <pre>data = data.dropna(subset=[label])\n</pre> data = data.dropna(subset=[label]) In\u00a0[\u00a0]: Copied! <pre>data = data[embedding_features + [label]]\nX = data[embedding_features]\ny = data[label]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</pre> data = data[embedding_features + [label]] X = data[embedding_features] y = data[label]  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 ) In\u00a0[\u00a0]: Copied! <pre>model = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n</pre> model = LinearRegression() model.fit(X_train, y_train) y_pred = model.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n</pre> evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred}) metrics = evaluate_model(evaluation_df) print(metrics) In\u00a0[\u00a0]: Copied! <pre>xy_lim = (0, 1_000_000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n</pre> xy_lim = (0, 1_000_000) plot_actual_vs_predicted(     evaluation_df,     xlim=xy_lim,     ylim=xy_lim,     title=\"Actual vs Predicted Home Values\",     x_label=\"Actual Home Value\",     y_label=\"Predicted Home Value\", ) <p></p> In\u00a0[\u00a0]: Copied! <pre>df = evaluation_df.join(gdf)\ndf[\"difference\"] = df[\"y_pred\"] - df[\"y\"]\nevaluation_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nevaluation_gdf.drop(columns=[\"category\", \"color\", column], inplace=True)\nevaluation_gdf.head()\n</pre> df = evaluation_df.join(gdf) df[\"difference\"] = df[\"y_pred\"] - df[\"y\"] evaluation_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\") evaluation_gdf.drop(columns=[\"category\", \"color\", column], inplace=True) evaluation_gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    evaluation_gdf,\n    cmap=\"Blues\",\n    column=\"y\",\n    legend_title=\"Actual Home Value\",\n    before_id=first_symbol_id,\n    name=\"Actual Home Value\",\n    extrude=True,\n    scale_factor=3,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     evaluation_gdf,     cmap=\"Blues\",     column=\"y\",     legend_title=\"Actual Home Value\",     before_id=first_symbol_id,     name=\"Actual Home Value\",     extrude=True,     scale_factor=3, ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    evaluation_gdf,\n    cmap=\"Blues\",\n    column=\"y_pred\",\n    legend_title=\"Predicted Home Value\",\n    before_id=first_symbol_id,\n    name=\"Predicted Home Value\",\n    extrude=True,\n    scale_factor=3,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     evaluation_gdf,     cmap=\"Blues\",     column=\"y_pred\",     legend_title=\"Predicted Home Value\",     before_id=first_symbol_id,     name=\"Predicted Home Value\",     extrude=True,     scale_factor=3, ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    evaluation_gdf,\n    cmap=\"coolwarm\",\n    column=\"difference\",\n    legend_title=\"y_pred-y\",\n    before_id=first_symbol_id,\n    name=\"Difference\",\n    extrude=True,\n    scale_factor=3,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     evaluation_gdf,     cmap=\"coolwarm\",     column=\"difference\",     legend_title=\"y_pred-y\",     before_id=first_symbol_id,     name=\"Difference\",     extrude=True,     scale_factor=3, ) m.add_layer_control() m <p></p>"},{"location":"PDFM/map_pdfm_features/#useful-resources","title":"Useful Resources\u00b6","text":"<ul> <li>Google's Population Dynamics Foundation Model (PDFM)</li> <li>Request access to PDFM embeddings here</li> <li>Zillow data can be accessed here</li> </ul>"},{"location":"PDFM/map_pdfm_features/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required libraries.</p>"},{"location":"PDFM/map_pdfm_features/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#download-zillow-data","title":"Download Zillow Data\u00b6","text":"<p>Download the Zillow home value data at the county level.</p>"},{"location":"PDFM/map_pdfm_features/#process-zillow-data","title":"Process Zillow Data\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#request-access-to-pdfm-embeddings","title":"Request access to PDFM Embeddings\u00b6","text":"<p>To request access to PDFM embeddings, please follow the instructions here.</p>"},{"location":"PDFM/map_pdfm_features/#load-county-boundaries","title":"Load county boundaries\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#join-home-value-data-and-county-boundaries","title":"Join home value data and county boundaries\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#visualize-home-values-in-2d","title":"Visualize home values in 2D\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#visualize-home-values-in-3d","title":"Visualize home values in 3D\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#load-pdfm-county-embeddings","title":"Load PDFM county embeddings\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#visualize-pdfm-features","title":"Visualize PDFM features\u00b6","text":"<p>Select any of the 329 PDFM features to visualize.</p>"},{"location":"PDFM/map_pdfm_features/#join-zillow-and-pdfm-data","title":"Join Zillow and PDFM Data\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#split-train-and-test-data","title":"Split Train and Test Data\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#fit-linear-regression-model","title":"Fit Linear Regression Model\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#evaluate-linear-regression-model","title":"Evaluate Linear Regression Model\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#join-predicted-values-with-county-boundaries","title":"Join predicted values with county boundaries\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#visualize-actual-home-values","title":"Visualize actual home values\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#visualize-predicted-home-values","title":"Visualize predicted home values\u00b6","text":""},{"location":"PDFM/map_pdfm_features/#visualize-difference-between-predicted-and-actual-home-values","title":"Visualize difference between predicted and actual home values\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/","title":"Pdfm superresolution","text":"In\u00a0[\u00a0]: Copied! <pre>import math\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn import metrics as skmetrics\nimport lightgbm as lgbm\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import preprocessing\nimport seaborn as sns\n</pre> import math import pandas as pd import numpy as np import geopandas as gpd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn import metrics as skmetrics import lightgbm as lgbm from sklearn.pipeline import make_pipeline from sklearn import preprocessing import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>BASE_PATH = \"./\"  # Set this to the path where your data files are located\n\ncounty_embeddings = pd.read_csv(BASE_PATH + \"county_embeddings.csv\").set_index(\"place\")\nzip_embeddings = pd.read_csv(BASE_PATH + \"zcta_embeddings.csv\").set_index(\"place\")\nembeddings = pd.concat([county_embeddings, zip_embeddings])\n</pre> BASE_PATH = \"./\"  # Set this to the path where your data files are located  county_embeddings = pd.read_csv(BASE_PATH + \"county_embeddings.csv\").set_index(\"place\") zip_embeddings = pd.read_csv(BASE_PATH + \"zcta_embeddings.csv\").set_index(\"place\") embeddings = pd.concat([county_embeddings, zip_embeddings]) In\u00a0[\u00a0]: Copied! <pre>embedding_features = [f\"feature{x}\" for x in range(330)]\nembeddings.head(2)\n</pre> embedding_features = [f\"feature{x}\" for x in range(330)] embeddings.head(2) In\u00a0[\u00a0]: Copied! <pre>zhvi_county_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\"\nzhvi_zipcode_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_zipcode.csv\"\n</pre> zhvi_county_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\" zhvi_zipcode_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_zipcode.csv\" In\u00a0[\u00a0]: Copied! <pre>zhvi_county_df = pd.read_csv(\n    zhvi_county_url, dtype={\"StateCodeFIPS\": \"string\", \"MunicipalCodeFIPS\": \"string\"}\n)\nzhvi_county_df[\"place\"] = (\n    \"geoId/\" + zhvi_county_df[\"StateCodeFIPS\"] + zhvi_county_df[\"MunicipalCodeFIPS\"]\n)\nzhvi_county_df = zhvi_county_df.set_index(\"place\")\nzhvi_county_df.head()\n</pre> zhvi_county_df = pd.read_csv(     zhvi_county_url, dtype={\"StateCodeFIPS\": \"string\", \"MunicipalCodeFIPS\": \"string\"} ) zhvi_county_df[\"place\"] = (     \"geoId/\" + zhvi_county_df[\"StateCodeFIPS\"] + zhvi_county_df[\"MunicipalCodeFIPS\"] ) zhvi_county_df = zhvi_county_df.set_index(\"place\") zhvi_county_df.head() In\u00a0[\u00a0]: Copied! <pre>zhvi_zipcode_df = pd.read_csv(zhvi_zipcode_url, dtype={\"RegionName\": \"string\"})\nzhvi_zipcode_df[\"place\"] = zhvi_zipcode_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\")\nzhvi_zipcode_df = zhvi_zipcode_df.set_index(\"place\")\nzhvi_zipcode_df.head()\n</pre> zhvi_zipcode_df = pd.read_csv(zhvi_zipcode_url, dtype={\"RegionName\": \"string\"}) zhvi_zipcode_df[\"place\"] = zhvi_zipcode_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\") zhvi_zipcode_df = zhvi_zipcode_df.set_index(\"place\") zhvi_zipcode_df.head() In\u00a0[\u00a0]: Copied! <pre>zhvi_df = pd.concat([zhvi_county_df, zhvi_zipcode_df])\nzhvi_df.head()\n</pre> zhvi_df = pd.concat([zhvi_county_df, zhvi_zipcode_df]) zhvi_df.head() In\u00a0[\u00a0]: Copied! <pre>zhvi_df[-5:]\n</pre> zhvi_df[-5:] In\u00a0[\u00a0]: Copied! <pre>len(zhvi_df)\n</pre> len(zhvi_df) In\u00a0[\u00a0]: Copied! <pre>df = embeddings.join(zhvi_df, how=\"inner\")\ndf.head()\n</pre> df = embeddings.join(zhvi_df, how=\"inner\") df.head() In\u00a0[\u00a0]: Copied! <pre>df[-3:]\n</pre> df[-3:] In\u00a0[\u00a0]: Copied! <pre>county_geo = gpd.read_file(BASE_PATH + \"county.geojson\").set_index(\"place\")\nzip_geo = gpd.read_file(BASE_PATH + \"zcta.geojson\").set_index(\"place\")\n</pre> county_geo = gpd.read_file(BASE_PATH + \"county.geojson\").set_index(\"place\") zip_geo = gpd.read_file(BASE_PATH + \"zcta.geojson\").set_index(\"place\") In\u00a0[\u00a0]: Copied! <pre>geo = pd.concat([county_geo, zip_geo])\nembeddings = gpd.GeoDataFrame(embeddings, geometry=geo.geometry)\nembeddings.shape\n</pre> geo = pd.concat([county_geo, zip_geo]) embeddings = gpd.GeoDataFrame(embeddings, geometry=geo.geometry) embeddings.shape In\u00a0[\u00a0]: Copied! <pre>df = embeddings.join(zhvi_df).set_geometry(\"geometry\")\ndf.head(1)\n</pre> df = embeddings.join(zhvi_df).set_geometry(\"geometry\") df.head(1) In\u00a0[\u00a0]: Copied! <pre>df[\"county_id\"] = df[\"county\"] + df[\"state\"]\n</pre> df[\"county_id\"] = df[\"county\"] + df[\"state\"] In\u00a0[\u00a0]: Copied! <pre>def get_locale(df, index, states=None, counties=None):\n    df = df[df.index.isin(index)]\n    if not states and not counties:\n        return df\n    filter = df.state.isin(states)\n    if counties:\n        filter &amp;= df.county.isin(counties)\n    return df[filter]\n</pre> def get_locale(df, index, states=None, counties=None):     df = df[df.index.isin(index)]     if not states and not counties:         return df     filter = df.state.isin(states)     if counties:         filter &amp;= df.county.isin(counties)     return df[filter] In\u00a0[\u00a0]: Copied! <pre># @title Map out an embedding dimension feature0 spatially across all counties in US\nfeature = embedding_features[300]\nax = get_locale(embeddings, embeddings.index).plot(feature)\n_ = ax.set_title(feature + \" in counties\")\n</pre> # @title Map out an embedding dimension feature0 spatially across all counties in US feature = embedding_features[300] ax = get_locale(embeddings, embeddings.index).plot(feature) _ = ax.set_title(feature + \" in counties\") In\u00a0[\u00a0]: Copied! <pre># @title Map out an embedding dimension feature0 spatially across all counties and zipcodes in NY state\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nstate = \"NY\"\nget_locale(embeddings, county_embeddings.index, states=[state]).plot(feature, ax=ax[0])\nget_locale(embeddings, zip_embeddings.index, states=[state]).plot(feature, ax=ax[1])\nfig.suptitle(f\"{feature} in {state}\")\nax[0].set(title=\"counties\")\nax[1].set(title=\"zip codes\")\nplt.setp(ax, xticks=[], yticks=[])\nfig.tight_layout()\n</pre> # @title Map out an embedding dimension feature0 spatially across all counties and zipcodes in NY state fig, ax = plt.subplots(1, 2, figsize=(8, 4)) state = \"NY\" get_locale(embeddings, county_embeddings.index, states=[state]).plot(feature, ax=ax[0]) get_locale(embeddings, zip_embeddings.index, states=[state]).plot(feature, ax=ax[1]) fig.suptitle(f\"{feature} in {state}\") ax[0].set(title=\"counties\") ax[1].set(title=\"zip codes\") plt.setp(ax, xticks=[], yticks=[]) fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>def evaluate(df: pd.DataFrame) -&gt; dict:\n    \"\"\"Evaluates the model performance on the given dataframe.\n\n    Args:\n        df: A pandas DataFrame with columns 'y' and 'y_pred'.\n\n    Returns:\n        A dictionary of performance metrics.\n    \"\"\"\n    # Ensure necessary columns exist and drop rows with NaN or zero in 'y'\n    if not {\"y\", \"y_pred\"}.issubset(df.columns):\n        raise ValueError(\"DataFrame must contain 'y' and 'y_pred' columns\")\n\n    df = df.dropna(subset=[\"y\", \"y_pred\"])\n    df = df[df[\"y\"] != 0]\n\n    r2 = skmetrics.r2_score(df[\"y\"], df[\"y_pred\"])\n    correlation = float(df[\"y\"].corr(df[\"y_pred\"]))\n    rmse = math.sqrt(skmetrics.mean_squared_error(df[\"y\"], df[\"y_pred\"]))\n    mae = float(skmetrics.mean_absolute_error(df[\"y\"], df[\"y_pred\"]))\n    mape = float(skmetrics.mean_absolute_percentage_error(df[\"y\"], df[\"y_pred\"]))\n\n    return {\n        \"r2\": r2,\n        \"rmse\": rmse,\n        \"mae\": mae,\n        \"mape\": mape,\n        \"correlation\": correlation,\n    }\n\n\ndef subset_eval(\n    label: str,\n    county_name: str,\n    state: str,\n    gpred: gpd.GeoDataFrame,\n    visualize: bool = True,\n    cmap: str = \"Greys\",\n) -&gt; dict:\n    \"\"\"Runs intra-county or intra-state evaluation and visualizes the results.\n\n    Args:\n        label: The label for the title of the visualization.\n        county_name: The specific county name to filter.\n        state: The specific state name to filter.\n        gpred: GeoDataFrame containing 'y', 'y_pred', 'state', and 'county' columns.\n        visualize: Whether to display visualizations.\n        cmap: Colormap for visualizations.\n\n    Returns:\n        A dictionary of performance metrics.\n    \"\"\"\n    # Apply filters based on state and county name\n    subset = gpred.copy()\n    if state:\n        subset = subset[subset[\"state\"] == state]\n    if county_name:\n        subset = subset[subset[\"county\"] == county_name]\n\n    # Drop rows where 'y' is NaN\n    subset = subset.dropna(subset=[\"y\", \"y_pred\"])\n    eval_metrics = evaluate(subset)\n\n    if visualize:\n        _, ax = plt.subplots(1, 3, figsize=(12, 4))\n\n        # Scatter plot of predicted vs actual\n        subset.plot.scatter(\"y\", \"y_pred\", alpha=0.8, ax=ax[2], color=\"darkgray\")\n        x0, x1 = (\n            subset[[\"y\", \"y_pred\"]].min().min(),\n            subset[[\"y\", \"y_pred\"]].max().max(),\n        )\n        ax[2].plot([x0, x1], [x0, x1], ls=\"--\", color=\"black\")\n        ax[2].set_title(\n            f'r={eval_metrics[\"correlation\"]:.2f}, mae={eval_metrics[\"mae\"]:.2f}'\n        )\n\n        # Maps of actual and predicted values\n        subset.plot(\n            \"y\",\n            legend=True,\n            ax=ax[0],\n            vmin=x0,\n            vmax=x1,\n            cmap=cmap,\n            legend_kwds={\"fraction\": 0.02, \"pad\": 0.05},\n        )\n        ax[0].set_title(\"Actual\")\n        subset.plot(\"y_pred\", legend=False, ax=ax[1], vmin=x0, vmax=x1, cmap=cmap)\n        ax[1].set_title(\"Predicted\")\n\n        plt.setp(ax[:2], xticks=[], yticks=[])\n        plt.suptitle(f\"{label} - {county_name}, {state}\")\n        plt.tight_layout()\n\n    return eval_metrics\n\n\ndef make_predictions_df(\n    predictions: np.ndarray, test_df: gpd.GeoDataFrame, label: str\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Creates a GeoDataFrame with predictions, true labels, and geographic info.\n\n    Args:\n        predictions: A sequence of predictions.\n        test_df: The original test GeoDataFrame that the predictions are based on.\n        label: The column name for the true label in `test_df`.\n\n    Returns:\n        A GeoDataFrame for evaluation and visualizations.\n    \"\"\"\n    if label not in test_df.columns:\n        raise ValueError(\n            f\"The specified label '{label}' does not exist in test_df columns.\"\n        )\n\n    df_predictions = pd.DataFrame(\n        {\"y\": test_df[label], \"y_pred\": predictions}, index=test_df.index\n    )\n    return test_df[[\"geometry\", \"state\", \"county\"]].join(df_predictions)\n</pre> def evaluate(df: pd.DataFrame) -&gt; dict:     \"\"\"Evaluates the model performance on the given dataframe.      Args:         df: A pandas DataFrame with columns 'y' and 'y_pred'.      Returns:         A dictionary of performance metrics.     \"\"\"     # Ensure necessary columns exist and drop rows with NaN or zero in 'y'     if not {\"y\", \"y_pred\"}.issubset(df.columns):         raise ValueError(\"DataFrame must contain 'y' and 'y_pred' columns\")      df = df.dropna(subset=[\"y\", \"y_pred\"])     df = df[df[\"y\"] != 0]      r2 = skmetrics.r2_score(df[\"y\"], df[\"y_pred\"])     correlation = float(df[\"y\"].corr(df[\"y_pred\"]))     rmse = math.sqrt(skmetrics.mean_squared_error(df[\"y\"], df[\"y_pred\"]))     mae = float(skmetrics.mean_absolute_error(df[\"y\"], df[\"y_pred\"]))     mape = float(skmetrics.mean_absolute_percentage_error(df[\"y\"], df[\"y_pred\"]))      return {         \"r2\": r2,         \"rmse\": rmse,         \"mae\": mae,         \"mape\": mape,         \"correlation\": correlation,     }   def subset_eval(     label: str,     county_name: str,     state: str,     gpred: gpd.GeoDataFrame,     visualize: bool = True,     cmap: str = \"Greys\", ) -&gt; dict:     \"\"\"Runs intra-county or intra-state evaluation and visualizes the results.      Args:         label: The label for the title of the visualization.         county_name: The specific county name to filter.         state: The specific state name to filter.         gpred: GeoDataFrame containing 'y', 'y_pred', 'state', and 'county' columns.         visualize: Whether to display visualizations.         cmap: Colormap for visualizations.      Returns:         A dictionary of performance metrics.     \"\"\"     # Apply filters based on state and county name     subset = gpred.copy()     if state:         subset = subset[subset[\"state\"] == state]     if county_name:         subset = subset[subset[\"county\"] == county_name]      # Drop rows where 'y' is NaN     subset = subset.dropna(subset=[\"y\", \"y_pred\"])     eval_metrics = evaluate(subset)      if visualize:         _, ax = plt.subplots(1, 3, figsize=(12, 4))          # Scatter plot of predicted vs actual         subset.plot.scatter(\"y\", \"y_pred\", alpha=0.8, ax=ax[2], color=\"darkgray\")         x0, x1 = (             subset[[\"y\", \"y_pred\"]].min().min(),             subset[[\"y\", \"y_pred\"]].max().max(),         )         ax[2].plot([x0, x1], [x0, x1], ls=\"--\", color=\"black\")         ax[2].set_title(             f'r={eval_metrics[\"correlation\"]:.2f}, mae={eval_metrics[\"mae\"]:.2f}'         )          # Maps of actual and predicted values         subset.plot(             \"y\",             legend=True,             ax=ax[0],             vmin=x0,             vmax=x1,             cmap=cmap,             legend_kwds={\"fraction\": 0.02, \"pad\": 0.05},         )         ax[0].set_title(\"Actual\")         subset.plot(\"y_pred\", legend=False, ax=ax[1], vmin=x0, vmax=x1, cmap=cmap)         ax[1].set_title(\"Predicted\")          plt.setp(ax[:2], xticks=[], yticks=[])         plt.suptitle(f\"{label} - {county_name}, {state}\")         plt.tight_layout()      return eval_metrics   def make_predictions_df(     predictions: np.ndarray, test_df: gpd.GeoDataFrame, label: str ) -&gt; gpd.GeoDataFrame:     \"\"\"Creates a GeoDataFrame with predictions, true labels, and geographic info.      Args:         predictions: A sequence of predictions.         test_df: The original test GeoDataFrame that the predictions are based on.         label: The column name for the true label in `test_df`.      Returns:         A GeoDataFrame for evaluation and visualizations.     \"\"\"     if label not in test_df.columns:         raise ValueError(             f\"The specified label '{label}' does not exist in test_df columns.\"         )      df_predictions = pd.DataFrame(         {\"y\": test_df[label], \"y_pred\": predictions}, index=test_df.index     )     return test_df[[\"geometry\", \"state\", \"county\"]].join(df_predictions) In\u00a0[\u00a0]: Copied! <pre># @title Train on counties and predict for zip codes\nlabel = \"2025-01-31\"\ndata = df[df[label].notna()]\ntrain = data[data.index.isin(county_geo.index)]\ntest = data[data.index.isin(zip_geo.index)]\n</pre> # @title Train on counties and predict for zip codes label = \"2025-01-31\" data = df[df[label].notna()] train = data[data.index.isin(county_geo.index)] test = data[data.index.isin(zip_geo.index)] In\u00a0[\u00a0]: Copied! <pre>len(train), len(test)\n</pre> len(train), len(test) In\u00a0[\u00a0]: Copied! <pre>train.head()\n</pre> train.head() In\u00a0[\u00a0]: Copied! <pre>test.head()\n</pre> test.head() In\u00a0[\u00a0]: Copied! <pre>model = Ridge()\nmodel.fit(train[embedding_features], train[label])\npredictions = model.predict(test[embedding_features])\ngdf_predictions = make_predictions_df(predictions, test, label)\nevaluate(gdf_predictions)\n</pre> model = Ridge() model.fit(train[embedding_features], train[label]) predictions = model.predict(test[embedding_features]) gdf_predictions = make_predictions_df(predictions, test, label) evaluate(gdf_predictions) In\u00a0[\u00a0]: Copied! <pre># @title Visualize some test set predictions\n_ = subset_eval(label, \"Harris County\", \"TX\", gdf_predictions, cmap=\"Blues\")\n_ = subset_eval(label, \"Greenville County\", \"SC\", gdf_predictions, cmap=\"Blues\")\n</pre> # @title Visualize some test set predictions _ = subset_eval(label, \"Harris County\", \"TX\", gdf_predictions, cmap=\"Blues\") _ = subset_eval(label, \"Greenville County\", \"SC\", gdf_predictions, cmap=\"Blues\") In\u00a0[\u00a0]: Copied! <pre># @title Evaluate over a state by setting the county to an empty string.\n_ = subset_eval(label, \"\", \"NY\", gdf_predictions, cmap=\"Blues\")\n</pre> # @title Evaluate over a state by setting the county to an empty string. _ = subset_eval(label, \"\", \"NY\", gdf_predictions, cmap=\"Blues\") In\u00a0[\u00a0]: Copied! <pre># @title train on zip codes in 20% of the counties, test on the remaining 80%.\n\n\ndef get_train_test_split(training_fraction=0.8):\n    data = df[df.index.isin(zip_embeddings.index)].copy()\n    # Split the zip codes by county into train/test sets.\n    train_counties = (\n        data.drop_duplicates(\"county_id\").sample(frac=training_fraction).county_id\n    )\n    train = data[data.county_id.isin(train_counties)]\n    test = data[~data.index.isin(train.index)]\n    print(\n        \"# training counties:\",\n        len(train_counties),\n        \"\\n# training zip codes:\",\n        train.shape[0],\n        \"\\n# test zip codes:\",\n        test.shape[0],\n    )\n    return train, test\n\n\ndef run_imputation_model(\n    train, test, label, min_population=500, model_class=Ridge, model_kwargs={}\n):\n    train = train[(train.population &gt;= min_population) &amp; train[label].notna()]\n    test = test[(test.population &gt;= min_population) &amp; test[label].notna()]\n    model = make_pipeline(preprocessing.MinMaxScaler(), model_class(**model_kwargs))\n    model.fit(train[embedding_features], train[label])\n    predictions = model.predict(test[embedding_features])\n    gdf_predictions = make_predictions_df(predictions, test, label)\n    results = evaluate(gdf_predictions)\n    return model, results\n\n\n# Increasing this value generally improves performance.\ntraining_fraction = 0.2\nlabel = \"2025-01-31\"\ntrain, test = get_train_test_split(training_fraction)\nmodel, results = run_imputation_model(train, test, label)\nresults\n</pre> # @title train on zip codes in 20% of the counties, test on the remaining 80%.   def get_train_test_split(training_fraction=0.8):     data = df[df.index.isin(zip_embeddings.index)].copy()     # Split the zip codes by county into train/test sets.     train_counties = (         data.drop_duplicates(\"county_id\").sample(frac=training_fraction).county_id     )     train = data[data.county_id.isin(train_counties)]     test = data[~data.index.isin(train.index)]     print(         \"# training counties:\",         len(train_counties),         \"\\n# training zip codes:\",         train.shape[0],         \"\\n# test zip codes:\",         test.shape[0],     )     return train, test   def run_imputation_model(     train, test, label, min_population=500, model_class=Ridge, model_kwargs={} ):     train = train[(train.population &gt;= min_population) &amp; train[label].notna()]     test = test[(test.population &gt;= min_population) &amp; test[label].notna()]     model = make_pipeline(preprocessing.MinMaxScaler(), model_class(**model_kwargs))     model.fit(train[embedding_features], train[label])     predictions = model.predict(test[embedding_features])     gdf_predictions = make_predictions_df(predictions, test, label)     results = evaluate(gdf_predictions)     return model, results   # Increasing this value generally improves performance. training_fraction = 0.2 label = \"2025-01-31\" train, test = get_train_test_split(training_fraction) model, results = run_imputation_model(train, test, label) results In\u00a0[\u00a0]: Copied! <pre># @title Visualize a few counties from the test set.\ntest_counties = test.county_id.unique()\nlarge_counties = (\n    df[df.county_id.isin(test_counties)]\n    .sort_values(\"population\", ascending=False)[[\"state\", \"county\", \"population\"]]\n    .head(4)\n)\nfor _, row in large_counties.iterrows():\n    _ = subset_eval(label, row.county, row.state, gdf_predictions, cmap=\"Blues\")\n</pre> # @title Visualize a few counties from the test set. test_counties = test.county_id.unique() large_counties = (     df[df.county_id.isin(test_counties)]     .sort_values(\"population\", ascending=False)[[\"state\", \"county\", \"population\"]]     .head(4) ) for _, row in large_counties.iterrows():     _ = subset_eval(label, row.county, row.state, gdf_predictions, cmap=\"Blues\") In\u00a0[\u00a0]: Copied! <pre># @title Try other labels.\nlabels = [\n    \"2024-01-31\",\n    \"2024-02-29\",\n    \"2024-03-31\",\n    \"2024-04-30\",\n    \"2024-05-31\",\n    \"2024-06-30\",\n    \"2024-07-31\",\n    \"2024-08-31\",\n    \"2024-09-30\",\n    \"2024-10-31\",\n    \"2024-11-30\",\n    \"2024-12-31\",\n]\ntrain, test = get_train_test_split(0.8)\nmodels_by_label = {}\nmetrics_df = pd.DataFrame(\n    columns=[\"label\", \"correlation\", \"r2\", \"rmse\", \"mae\", \"mape\", \"model\"]\n)\nfor label in labels:\n    models_by_label[label], results = run_imputation_model(train, test, label)\n    results[\"label\"] = label\n    results[\"model\"] = \"linear\"\n    metrics_df.loc[len(metrics_df)] = results\n\nmetrics_df.round(3)\n</pre> # @title Try other labels. labels = [     \"2024-01-31\",     \"2024-02-29\",     \"2024-03-31\",     \"2024-04-30\",     \"2024-05-31\",     \"2024-06-30\",     \"2024-07-31\",     \"2024-08-31\",     \"2024-09-30\",     \"2024-10-31\",     \"2024-11-30\",     \"2024-12-31\", ] train, test = get_train_test_split(0.8) models_by_label = {} metrics_df = pd.DataFrame(     columns=[\"label\", \"correlation\", \"r2\", \"rmse\", \"mae\", \"mape\", \"model\"] ) for label in labels:     models_by_label[label], results = run_imputation_model(train, test, label)     results[\"label\"] = label     results[\"model\"] = \"linear\"     metrics_df.loc[len(metrics_df)] = results  metrics_df.round(3) In\u00a0[\u00a0]: Copied! <pre># @title Try LightGBM models instead of linear.\n\n# This will take a few minutes to run.\nmodels_by_label_lgbm = {}\nmetrics_df_lgbm = pd.DataFrame(\n    columns=[\"label\", \"r2\", \"rmse\", \"mae\", \"mape\", \"correlation\", \"model\"]\n)\nfor label in labels:\n    models_by_label_lgbm[label], results = run_imputation_model(\n        train,\n        test,\n        label,\n        model_class=lgbm.LGBMRegressor,\n        model_kwargs={\n            \"min_child_samples\": 40,\n            \"importance_type\": \"gain\",\n            \"n_estimators\": 400,\n            \"learning_rate\": 0.04,\n            \"force_col_wise\": True,\n        },\n    )\n    results[\"label\"] = label\n    results[\"model\"] = \"lgbm\"\n    metrics_df_lgbm.loc[len(metrics_df_lgbm)] = results\n\nmetrics_df_lgbm.round(3)\n</pre> # @title Try LightGBM models instead of linear.  # This will take a few minutes to run. models_by_label_lgbm = {} metrics_df_lgbm = pd.DataFrame(     columns=[\"label\", \"r2\", \"rmse\", \"mae\", \"mape\", \"correlation\", \"model\"] ) for label in labels:     models_by_label_lgbm[label], results = run_imputation_model(         train,         test,         label,         model_class=lgbm.LGBMRegressor,         model_kwargs={             \"min_child_samples\": 40,             \"importance_type\": \"gain\",             \"n_estimators\": 400,             \"learning_rate\": 0.04,             \"force_col_wise\": True,         },     )     results[\"label\"] = label     results[\"model\"] = \"lgbm\"     metrics_df_lgbm.loc[len(metrics_df_lgbm)] = results  metrics_df_lgbm.round(3) <p>The LGBM results are mostly comparable with the linear model. They can be improved with more iterations and lower learning rate. You can also try setting <code>feature_fraction=0.5</code>.</p> In\u00a0[\u00a0]: Copied! <pre># @title LightGBM feature importance\n\nfeatures = {\n    \"trends\": (128, embedding_features[:128]),\n    \"maps\": (128, embedding_features[128:256]),\n    \"weather\": (74, embedding_features[256:]),\n}\nall_importance = []\nfor label, model in models_by_label_lgbm.items():\n    importance = pd.DataFrame(\n        model[1].feature_importances_, index=embedding_features, columns=[\"importance\"]\n    )\n    importance[\"importance\"] = importance[\"importance\"].abs()\n    for feature, dims in features.items():\n        importance.loc[dims[1], \"feature\"] = feature\n    importance = importance.groupby(\"feature\").importance.sum().reset_index()\n    importance[\"importance\"] = importance.importance / importance.importance.sum() * 100\n    importance[\"label\"] = label\n    all_importance.append(importance)\nall_importance = pd.concat(all_importance)\n_, ax = plt.subplots(figsize=(10, 3))\nsns.barplot(\n    data=all_importance,\n    x=\"label\",\n    y=\"importance\",\n    hue=\"feature\",\n    hue_order=features.keys(),\n    ax=ax,\n)\n_ = plt.xticks(rotation=30)\n</pre> # @title LightGBM feature importance  features = {     \"trends\": (128, embedding_features[:128]),     \"maps\": (128, embedding_features[128:256]),     \"weather\": (74, embedding_features[256:]), } all_importance = [] for label, model in models_by_label_lgbm.items():     importance = pd.DataFrame(         model[1].feature_importances_, index=embedding_features, columns=[\"importance\"]     )     importance[\"importance\"] = importance[\"importance\"].abs()     for feature, dims in features.items():         importance.loc[dims[1], \"feature\"] = feature     importance = importance.groupby(\"feature\").importance.sum().reset_index()     importance[\"importance\"] = importance.importance / importance.importance.sum() * 100     importance[\"label\"] = label     all_importance.append(importance) all_importance = pd.concat(all_importance) _, ax = plt.subplots(figsize=(10, 3)) sns.barplot(     data=all_importance,     x=\"label\",     y=\"importance\",     hue=\"feature\",     hue_order=features.keys(),     ax=ax, ) _ = plt.xticks(rotation=30)"},{"location":"PDFM/pdfm_superresolution/#pdfm-super-resolution-with-zillow-housing-data","title":"PDFM Super Resolution with Zillow Housing Data\u00b6","text":"<p>This notebook is adapted from the PDFM notebook example here. Credits to the original authors.</p>"},{"location":"PDFM/pdfm_superresolution/#data-preparation","title":"Data Preparation\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/#step-1-download-a-csv-file-of-the-embeddings-using-this-link","title":"Step 1: Download a csv file of the embeddings using this link.\u00b6","text":"<p>The county and ZCTA (zipcode census tabulation area) embeddings are available in different files.</p> <p>Here we assume that you have obtained the embeddings and uploaded them to a Google Drive directory called <code>pdfm_embeddings/v0/us</code>.</p>"},{"location":"PDFM/pdfm_superresolution/#step-2-download-and-load-a-few-variables-from-github","title":"Step 2: Download and load a few variables from GitHub\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/#data-visualizations","title":"Data Visualizations\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/#download-the-county-and-zcta-zipcode-census-tabulation-area-level-geojson-file","title":"Download the county and zcta (Zipcode census tabulation area) level geojson file.\u00b6","text":"<p>The county and zcta level geojson file are available in the same folder as the embeddings. Download the geojson file and upload to Google Colab.</p>"},{"location":"PDFM/pdfm_superresolution/#map-out-an-embedding-dimension-spatially","title":"Map out an embedding dimension spatially\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/#applying-the-embeddings-in-a-prediction-task","title":"Applying the embeddings in a prediction task\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/#superresolution-train-the-model-on-counties-and-make-predictions-for-zip-code","title":"Superresolution - Train the model on counties and make predictions for zip code\u00b6","text":""},{"location":"PDFM/pdfm_superresolution/#imputation-zip-zip","title":"Imputation - zip -&gt; zip\u00b6","text":"<p>Train on zipcodes in a subset of counties.</p>"},{"location":"PDFM/zillow_home_value/","title":"Zillow home value","text":"<p>Predicting US Housing Prices at the Zip Code Level Using Google's Population Dynamics Foundation Model and Zillow Data</p> In\u00a0[\u00a0]: Copied! <pre># %pip install leafmap scikit-learn\n</pre> # %pip install leafmap scikit-learn In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\n</pre> import os import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor from leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file In\u00a0[\u00a0]: Copied! <pre>zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_zipcode.csv\"\nzhvi_file = \"data/zillow_home_value_index_by_zipcode.csv\"\n</pre> zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_zipcode.csv\" zhvi_file = \"data/zillow_home_value_index_by_zipcode.csv\" In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(zhvi_file):\n    download_file(zhvi_url, zhvi_file)\n</pre> if not os.path.exists(zhvi_file):     download_file(zhvi_url, zhvi_file) In\u00a0[\u00a0]: Copied! <pre>zhvi_df = pd.read_csv(zhvi_file, dtype={\"RegionName\": \"string\"})\nzhvi_df.index = zhvi_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\")\nzhvi_df.head()\n</pre> zhvi_df = pd.read_csv(zhvi_file, dtype={\"RegionName\": \"string\"}) zhvi_df.index = zhvi_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\") zhvi_df.head() In\u00a0[\u00a0]: Copied! <pre>embeddings_file_path = \"data/zcta_embeddings.csv\"\n</pre> embeddings_file_path = \"data/zcta_embeddings.csv\" <p>To request access to PDFM embeddings, please follow the instructions here.</p> In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(embeddings_file_path):\n    raise FileNotFoundError(\"Please request the embeddings from Google\")\n</pre> if not os.path.exists(embeddings_file_path):     raise FileNotFoundError(\"Please request the embeddings from Google\") In\u00a0[\u00a0]: Copied! <pre>zipcode_embeddings = pd.read_csv(embeddings_file_path).set_index(\"place\")\nzipcode_embeddings.head()\n</pre> zipcode_embeddings = pd.read_csv(embeddings_file_path).set_index(\"place\") zipcode_embeddings.head() In\u00a0[\u00a0]: Copied! <pre>data = zhvi_df.join(zipcode_embeddings, how=\"inner\")\ndata.head()\n</pre> data = zhvi_df.join(zipcode_embeddings, how=\"inner\") data.head() In\u00a0[\u00a0]: Copied! <pre>embedding_features = [f\"feature{x}\" for x in range(330)]\nlabel = \"2024-10-31\"\n</pre> embedding_features = [f\"feature{x}\" for x in range(330)] label = \"2024-10-31\" In\u00a0[\u00a0]: Copied! <pre>data = data.dropna(subset=[label])\n</pre> data = data.dropna(subset=[label]) In\u00a0[\u00a0]: Copied! <pre>data = data[embedding_features + [label]]\nX = data[embedding_features]\ny = data[label]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</pre> data = data[embedding_features + [label]] X = data[embedding_features] y = data[label]  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 ) In\u00a0[\u00a0]: Copied! <pre># Initialize and train a simple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n</pre> # Initialize and train a simple linear regression model model = LinearRegression() model.fit(X_train, y_train)  # Make predictions y_pred = model.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n</pre> evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred}) metrics = evaluate_model(evaluation_df) print(metrics) In\u00a0[\u00a0]: Copied! <pre>xy_lim = (0, 3_000_000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n</pre> xy_lim = (0, 3_000_000) plot_actual_vs_predicted(     evaluation_df,     xlim=xy_lim,     ylim=xy_lim,     title=\"Actual vs Predicted Home Values\",     x_label=\"Actual Home Value\",     y_label=\"Predicted Home Value\", ) <p></p> In\u00a0[\u00a0]: Copied! <pre>k = 5\nmodel = KNeighborsRegressor(n_neighbors=k)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n</pre> k = 5 model = KNeighborsRegressor(n_neighbors=k) model.fit(X_train, y_train)  y_pred = model.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\n# Evaluate the model\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n</pre> evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred}) # Evaluate the model metrics = evaluate_model(evaluation_df) print(metrics) In\u00a0[\u00a0]: Copied! <pre>plot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n</pre> plot_actual_vs_predicted(     evaluation_df,     xlim=xy_lim,     ylim=xy_lim,     title=\"Actual vs Predicted Home Values\",     x_label=\"Actual Home Value\",     y_label=\"Predicted Home Value\", ) <p></p>"},{"location":"PDFM/zillow_home_value/#useful-resources","title":"Useful Resources\u00b6","text":"<ul> <li>Google's Population Dynamics Foundation Model (PDFM)</li> <li>Request access to PDFM embeddings here</li> <li>Zillow data can be accessed here</li> </ul>"},{"location":"PDFM/zillow_home_value/#acknowledgements","title":"Acknowledgements\u00b6","text":"<p>This notebook is adapted from the PDFM tutorial. Credit goes to the authors of the PDFM tutorial.</p>"},{"location":"PDFM/zillow_home_value/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required libraries.</p>"},{"location":"PDFM/zillow_home_value/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"PDFM/zillow_home_value/#download-zillow-data","title":"Download Zillow Data\u00b6","text":""},{"location":"PDFM/zillow_home_value/#process-zillow-data","title":"Process Zillow Data\u00b6","text":""},{"location":"PDFM/zillow_home_value/#request-access-to-pdfm-embeddings","title":"Request access to PDFM Embeddings\u00b6","text":""},{"location":"PDFM/zillow_home_value/#load-pdfm-embeddings","title":"Load PDFM Embeddings\u00b6","text":""},{"location":"PDFM/zillow_home_value/#join-zillow-and-pdfm-data","title":"Join Zillow and PDFM Data\u00b6","text":""},{"location":"PDFM/zillow_home_value/#split-train-and-test-data","title":"Split Train and Test Data\u00b6","text":""},{"location":"PDFM/zillow_home_value/#fit-linear-regression-model","title":"Fit Linear Regression Model\u00b6","text":""},{"location":"PDFM/zillow_home_value/#evaluate-linear-regression-model","title":"Evaluate Linear Regression Model\u00b6","text":""},{"location":"PDFM/zillow_home_value/#fit-k-nearest-neighbors-model","title":"Fit K-Nearest Neighbors Model\u00b6","text":""},{"location":"PDFM/zillow_home_value/#evaluate-k-nearest-neighbors-model","title":"Evaluate K-Nearest Neighbors Model\u00b6","text":""},{"location":"workshops/AIforGood_2025/","title":"AIforGood 2025","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install \"leafmap[maplibre]\" scikit-learn\n</pre> %pip install \"leafmap[maplibre]\" scikit-learn In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\n</pre> import os import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor from leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file In\u00a0[\u00a0]: Copied! <pre>zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_zipcode.csv\"\nzhvi_file = \"zillow_home_value_index_by_zipcode.csv\"\n</pre> zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_zipcode.csv\" zhvi_file = \"zillow_home_value_index_by_zipcode.csv\" In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(zhvi_file):\n    download_file(zhvi_url, zhvi_file)\n</pre> if not os.path.exists(zhvi_file):     download_file(zhvi_url, zhvi_file) In\u00a0[\u00a0]: Copied! <pre>zhvi_df = pd.read_csv(zhvi_file, dtype={\"RegionName\": \"string\"})\nzhvi_df.index = zhvi_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\")\nzhvi_df.head()\n</pre> zhvi_df = pd.read_csv(zhvi_file, dtype={\"RegionName\": \"string\"}) zhvi_df.index = zhvi_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\") zhvi_df.head() In\u00a0[\u00a0]: Copied! <pre>embeddings_file_path = \"zcta_embeddings.csv\"\n</pre> embeddings_file_path = \"zcta_embeddings.csv\" In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(embeddings_file_path):\n    raise FileNotFoundError(\"Please request the embeddings from Google\")\n</pre> if not os.path.exists(embeddings_file_path):     raise FileNotFoundError(\"Please request the embeddings from Google\") In\u00a0[\u00a0]: Copied! <pre>zipcode_embeddings = pd.read_csv(embeddings_file_path).set_index(\"place\")\nzipcode_embeddings.head()\n</pre> zipcode_embeddings = pd.read_csv(embeddings_file_path).set_index(\"place\") zipcode_embeddings.head() In\u00a0[\u00a0]: Copied! <pre>data = zhvi_df.join(zipcode_embeddings, how=\"inner\")\ndata.head()\n</pre> data = zhvi_df.join(zipcode_embeddings, how=\"inner\") data.head() In\u00a0[\u00a0]: Copied! <pre>embedding_features = [f\"feature{x}\" for x in range(330)]\nlabel = \"2025-01-31\"\n</pre> embedding_features = [f\"feature{x}\" for x in range(330)] label = \"2025-01-31\" In\u00a0[\u00a0]: Copied! <pre>data = data.dropna(subset=[label])\n</pre> data = data.dropna(subset=[label]) In\u00a0[\u00a0]: Copied! <pre>data = data[embedding_features + [label]]\nX = data[embedding_features]\ny = data[label]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</pre> data = data[embedding_features + [label]] X = data[embedding_features] y = data[label]  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 ) In\u00a0[\u00a0]: Copied! <pre>X_train.head()\n</pre> X_train.head() In\u00a0[\u00a0]: Copied! <pre>y_train.head()\n</pre> y_train.head() In\u00a0[\u00a0]: Copied! <pre># Initialize and train a simple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n</pre> # Initialize and train a simple linear regression model model = LinearRegression() model.fit(X_train, y_train)  # Make predictions y_pred = model.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n</pre> evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred}) metrics = evaluate_model(evaluation_df) print(metrics) In\u00a0[\u00a0]: Copied! <pre>xy_lim = (0, 3_000_000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n</pre> xy_lim = (0, 3_000_000) plot_actual_vs_predicted(     evaluation_df,     xlim=xy_lim,     ylim=xy_lim,     title=\"Actual vs Predicted Home Values\",     x_label=\"Actual Home Value\",     y_label=\"Predicted Home Value\", ) In\u00a0[\u00a0]: Copied! <pre>k = 5\nmodel = KNeighborsRegressor(n_neighbors=k)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n</pre> k = 5 model = KNeighborsRegressor(n_neighbors=k) model.fit(X_train, y_train)  y_pred = model.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n</pre> evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred}) metrics = evaluate_model(evaluation_df) print(metrics) In\u00a0[\u00a0]: Copied! <pre>plot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n</pre> plot_actual_vs_predicted(     evaluation_df,     xlim=xy_lim,     ylim=xy_lim,     title=\"Actual vs Predicted Home Values\",     x_label=\"Actual Home Value\",     y_label=\"Predicted Home Value\", ) In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\nimport leafmap.maplibregl as leafmap\n</pre> import os import pandas as pd import geopandas as gpd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor from leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file import leafmap.maplibregl as leafmap In\u00a0[\u00a0]: Copied! <pre>zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\"\nzhvi_file = \"zillow_home_value_index_by_county.csv\"\n</pre> zhvi_url = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\" zhvi_file = \"zillow_home_value_index_by_county.csv\" In\u00a0[\u00a0]: Copied! <pre>if not os.path.exists(zhvi_file):\n    download_file(zhvi_url, zhvi_file)\n</pre> if not os.path.exists(zhvi_file):     download_file(zhvi_url, zhvi_file) In\u00a0[\u00a0]: Copied! <pre>zhvi_df = pd.read_csv(\n    zhvi_file, dtype={\"StateCodeFIPS\": \"string\", \"MunicipalCodeFIPS\": \"string\"}\n)\nzhvi_df.index = \"geoId/\" + zhvi_df[\"StateCodeFIPS\"] + zhvi_df[\"MunicipalCodeFIPS\"]\nzhvi_df.head()\n</pre> zhvi_df = pd.read_csv(     zhvi_file, dtype={\"StateCodeFIPS\": \"string\", \"MunicipalCodeFIPS\": \"string\"} ) zhvi_df.index = \"geoId/\" + zhvi_df[\"StateCodeFIPS\"] + zhvi_df[\"MunicipalCodeFIPS\"] zhvi_df.head() In\u00a0[\u00a0]: Copied! <pre>county_geojson = \"county.geojson\"\nif not os.path.exists(county_geojson):\n    raise FileNotFoundError(\"Please request the embeddings from Google\")\n</pre> county_geojson = \"county.geojson\" if not os.path.exists(county_geojson):     raise FileNotFoundError(\"Please request the embeddings from Google\") In\u00a0[\u00a0]: Copied! <pre>county_gdf = gpd.read_file(county_geojson)\ncounty_gdf.set_index(\"place\", inplace=True)\ncounty_gdf.head()\n</pre> county_gdf = gpd.read_file(county_geojson) county_gdf.set_index(\"place\", inplace=True) county_gdf.head() In\u00a0[\u00a0]: Copied! <pre>df = zhvi_df.join(county_gdf)\nzhvi_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nzhvi_gdf.head()\n</pre> df = zhvi_df.join(county_gdf) zhvi_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\") zhvi_gdf.head() In\u00a0[\u00a0]: Copied! <pre>column = \"2025-01-31\"\ngdf = zhvi_gdf[[\"RegionName\", \"State\", column, \"geometry\"]]\ngdf.head()\n</pre> column = \"2025-01-31\" gdf = zhvi_gdf[[\"RegionName\", \"State\", column, \"geometry\"]] gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\")\nfirst_symbol_id = m.find_first_symbol_layer()[\"id\"]\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=\"Median Home Value\",\n    name=\"Median Home Value\",\n    before_id=first_symbol_id,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\") first_symbol_id = m.find_first_symbol_layer()[\"id\"] m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=\"Median Home Value\",     name=\"Median Home Value\",     before_id=first_symbol_id, ) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=\"Median Home Value\",\n    extrude=True,\n    scale_factor=3,\n    before_id=first_symbol_id,\n    name=\"Median Home Value\",\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=\"Median Home Value\",     extrude=True,     scale_factor=3,     before_id=first_symbol_id,     name=\"Median Home Value\", ) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>embeddings_file_path = \"county_embeddings.csv\"\n</pre> embeddings_file_path = \"county_embeddings.csv\" In\u00a0[\u00a0]: Copied! <pre>embeddings_df = pd.read_csv(embeddings_file_path).set_index(\"place\")\nembeddings_df.head()\n</pre> embeddings_df = pd.read_csv(embeddings_file_path).set_index(\"place\") embeddings_df.head() In\u00a0[\u00a0]: Copied! <pre>df = embeddings_df.join(county_gdf)\nembeddings_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nembeddings_gdf.head()\n</pre> df = embeddings_df.join(county_gdf) embeddings_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\") embeddings_gdf.head() In\u00a0[\u00a0]: Copied! <pre>column = \"feature329\"  # Change this to the feature you want to use\ngdf = embeddings_gdf[[column, \"state\", \"county\", \"geometry\"]]\ngdf.head()\n</pre> column = \"feature329\"  # Change this to the feature you want to use gdf = embeddings_gdf[[column, \"state\", \"county\", \"geometry\"]] gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=column,\n    before_id=first_symbol_id,\n    name=column,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\") m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=column,     before_id=first_symbol_id,     name=column, ) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=column,\n    before_id=first_symbol_id,\n    name=column,\n    extrude=True,\n    scale_factor=0.00005,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     gdf,     cmap=\"Blues\",     column=column,     legend_title=column,     before_id=first_symbol_id,     name=column,     extrude=True,     scale_factor=0.00005, ) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>data = zhvi_df.join(embeddings_df, how=\"inner\")\ndata.head()\n</pre> data = zhvi_df.join(embeddings_df, how=\"inner\") data.head() In\u00a0[\u00a0]: Copied! <pre>embedding_features = [f\"feature{x}\" for x in range(330)]\nlabel = \"2025-01-31\"  # Change this to the date you want to predict\n</pre> embedding_features = [f\"feature{x}\" for x in range(330)] label = \"2025-01-31\"  # Change this to the date you want to predict In\u00a0[\u00a0]: Copied! <pre>data = data.dropna(subset=[label])\n</pre> data = data.dropna(subset=[label]) In\u00a0[\u00a0]: Copied! <pre>data = data[embedding_features + [label]]\nX = data[embedding_features]\ny = data[label]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</pre> data = data[embedding_features + [label]] X = data[embedding_features] y = data[label]  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, random_state=42 ) In\u00a0[\u00a0]: Copied! <pre>model = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n</pre> model = LinearRegression() model.fit(X_train, y_train) y_pred = model.predict(X_test) In\u00a0[\u00a0]: Copied! <pre>evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n</pre> evaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred}) metrics = evaluate_model(evaluation_df) print(metrics) In\u00a0[\u00a0]: Copied! <pre>xy_lim = (0, 1_000_000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n</pre> xy_lim = (0, 1_000_000) plot_actual_vs_predicted(     evaluation_df,     xlim=xy_lim,     ylim=xy_lim,     title=\"Actual vs Predicted Home Values\",     x_label=\"Actual Home Value\",     y_label=\"Predicted Home Value\", ) In\u00a0[\u00a0]: Copied! <pre>df = evaluation_df.join(gdf)\ndf[\"difference\"] = df[\"y_pred\"] - df[\"y\"]\nevaluation_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nevaluation_gdf.drop(columns=[\"category\", \"color\", column], inplace=True)\nevaluation_gdf.head()\n</pre> df = evaluation_df.join(gdf) df[\"difference\"] = df[\"y_pred\"] - df[\"y\"] evaluation_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\") evaluation_gdf.drop(columns=[\"category\", \"color\", column], inplace=True) evaluation_gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    evaluation_gdf,\n    cmap=\"Blues\",\n    column=\"y\",\n    legend_title=\"Actual Home Value\",\n    before_id=first_symbol_id,\n    name=\"Actual Home Value\",\n    extrude=True,\n    scale_factor=3,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     evaluation_gdf,     cmap=\"Blues\",     column=\"y\",     legend_title=\"Actual Home Value\",     before_id=first_symbol_id,     name=\"Actual Home Value\",     extrude=True,     scale_factor=3, ) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    evaluation_gdf,\n    cmap=\"Blues\",\n    column=\"y_pred\",\n    legend_title=\"Predicted Home Value\",\n    before_id=first_symbol_id,\n    name=\"Predicted Home Value\",\n    extrude=True,\n    scale_factor=3,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     evaluation_gdf,     cmap=\"Blues\",     column=\"y_pred\",     legend_title=\"Predicted Home Value\",     before_id=first_symbol_id,     name=\"Predicted Home Value\",     extrude=True,     scale_factor=3, ) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    evaluation_gdf,\n    cmap=\"coolwarm\",\n    column=\"difference\",\n    legend_title=\"y_pred-y\",\n    before_id=first_symbol_id,\n    name=\"Difference\",\n    extrude=True,\n    scale_factor=3,\n)\nm.add_layer_control()\nm\n</pre> m = leafmap.Map(style=\"liberty\", pitch=60) m.add_data(     evaluation_gdf,     cmap=\"coolwarm\",     column=\"difference\",     legend_title=\"y_pred-y\",     before_id=first_symbol_id,     name=\"Difference\",     extrude=True,     scale_factor=3, ) m.add_layer_control() m"},{"location":"workshops/AIforGood_2025/#ai-for-good-workshop-2025","title":"AI for Good Workshop 2025\u00b6","text":"<p>Join us for the AI for Good Workshop 2025, part of the UN's AI for Good workshop series! This workshop will take place online on February 18, 2025, from 9:00 AM to 10:30 AM EST. It is free and open to the public. Please register using this link: Modeling population dynamics with AI: A hands-on workshop with the Population Dynamics Foundation Model.</p>"},{"location":"workshops/AIforGood_2025/#overview","title":"Overview\u00b6","text":"<p>Explore the transformative potential of the Population Dynamics Foundation Model (PDFM), a cutting-edge AI model designed to capture complex, multidimensional interactions among human behaviors, environmental factors, and local contexts. This workshop provides an in-depth introduction to PDFM Embeddings and their applications in geospatial analysis, public health, and socioeconomic modeling.</p> <p>Participants will gain hands-on experience with PDFM Embeddings to perform advanced geospatial predictions and analyses while ensuring privacy through the use of aggregated data. Key components of the workshop include:</p> <ul> <li>Introduction to PDFM Embeddings: Delve into the model architecture of PDFM and discover how aggregated data (such as search trends, busyness levels, and weather conditions) generates location-specific embeddings.</li> <li>Data Preparation: Learn to integrate ground truth data, including health statistics and socioeconomic indicators, with PDFM Embeddings at the postal code or county level.</li> <li>Hands-On Exercises: Engage with interactive Colab notebooks to explore real-world applications, such as predicting housing prices using Zillow data and nighttime light predictions with Google Earth Engine data.</li> <li>Visualization and Interpretation: Analyze and visualize geospatial predictions and PDFM features in 3D, enhancing your ability to interpret complex datasets.</li> </ul> <p>By the end of this workshop, participants will have a strong foundation in utilizing PDFM Embeddings to address real-world geospatial challenges.</p>"},{"location":"workshops/AIforGood_2025/#target-audience","title":"Target audience\u00b6","text":"<p>This workshop is designed for data scientists, geospatial analysts, researchers, urban planners, and professionals in public health, economics, or environmental science who want to integrate AI into their workflows.</p>"},{"location":"workshops/AIforGood_2025/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>A Google Colab account</li> <li>Access to the PDFM embeddings</li> <li>Basic understanding of Python programming and geospatial data concepts is recommended</li> </ul>"},{"location":"workshops/AIforGood_2025/#recording","title":"Recording\u00b6","text":"<p>The recording of the workshop will be made available on YouTube after the event. Stay tuned for the link!</p>"},{"location":"workshops/AIforGood_2025/#environment-setup","title":"Environment setup\u00b6","text":""},{"location":"workshops/AIforGood_2025/#install-the-required-packages-locally","title":"Install the required packages locally\u00b6","text":"<p>If you are running this notebook locally, you can install the required packages using the following commands:</p> <pre>conda create -n sam python=3.12\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge leafmap maplibre scikit-learn\n</pre>"},{"location":"workshops/AIforGood_2025/#use-google-colab","title":"Use Google Colab\u00b6","text":"<p>If you are using Google Colab, run the following cell to install the required packages:</p>"},{"location":"workshops/AIforGood_2025/#predicting-us-housing-prices-using-pdfm-and-zillow-data","title":"Predicting US Housing Prices Using PDFM and Zillow Data\u00b6","text":"<p>To follow along with the workshop, you will need to have access to the PDFM embeddings. Please request access to the PDFM embeddings here. Download the embeddings and upload them to your Google Drive or Google Colab environment.</p> <p>This notebook is adapted from the PDFM tutorial. Credit goes to the authors of the PDFM tutorial.</p>"},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"workshops/AIforGood_2025/#download-zillow-data","title":"Download Zillow Data\u00b6","text":"<p>The Zillow housing data can be downloaded from the Zillow Research Data page. We will use the Zillow Home Value Index (ZHVI) data for single-family homes at the county level.</p>"},{"location":"workshops/AIforGood_2025/#process-zillow-data","title":"Process Zillow Data\u00b6","text":"<p>The Zillow ZHVI dataset contains a <code>RegionName</code> column that corresponds to the zip code. We need to format the zip code to match the PDFM embeddings' <code>postal_code</code> format, which looks like <code>zip/XXXXX</code>.</p>"},{"location":"workshops/AIforGood_2025/#request-access-to-pdfm-embeddings","title":"Request access to PDFM Embeddings\u00b6","text":"<p>Please request access to the PDFM embeddings here. Download the embeddings, unzip the file, and upload the following files to Google Colab:</p> <ul> <li><code>zcta_embeddings.csv</code></li> <li><code>county_embeddings.csv</code></li> <li><code>county.geojson</code></li> </ul>"},{"location":"workshops/AIforGood_2025/#load-pdfm-embeddings","title":"Load PDFM Embeddings\u00b6","text":"<p>We will load the PDFM embeddings from Google Colab where you saved the embeddings.</p>"},{"location":"workshops/AIforGood_2025/#join-zillow-and-pdfm-data","title":"Join Zillow and PDFM Data\u00b6","text":"<p>We will join the Zillow and PDFM data using the GeoDataFrame index.</p>"},{"location":"workshops/AIforGood_2025/#split-train-and-test-data","title":"Split Train and Test Data\u00b6","text":"<p>We will split the data into training and testing datasets using a 80-20 split. We will use the training data to train a machine learning model to predict housing prices.</p>"},{"location":"workshops/AIforGood_2025/#fit-linear-regression-model","title":"Fit Linear Regression Model\u00b6","text":"<p>We will fit a linear regression model to predict the Zillow Home Value Index (ZHVI) using the PDFM embeddings.</p>"},{"location":"workshops/AIforGood_2025/#evaluate-linear-regression-model","title":"Evaluate Linear Regression Model\u00b6","text":""},{"location":"workshops/AIforGood_2025/#fit-k-nearest-neighbors-model","title":"Fit K-Nearest Neighbors Model\u00b6","text":"<p>We will fit a K-Nearest Neighbors (KNN) model to predict the Zillow Home Value Index (ZHVI) using the PDFM embeddings.</p>"},{"location":"workshops/AIforGood_2025/#evaluate-k-nearest-neighbors-model","title":"Evaluate K-Nearest Neighbors Model\u00b6","text":""},{"location":"workshops/AIforGood_2025/#mapping-pdfm-features-and-predicted-housing-prices","title":"Mapping PDFM Features and Predicted Housing Prices\u00b6","text":""},{"location":"workshops/AIforGood_2025/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"workshops/AIforGood_2025/#download-zillow-data","title":"Download Zillow Data\u00b6","text":"<p>Download the Zillow home value data at the county level.</p>"},{"location":"workshops/AIforGood_2025/#process-zillow-data","title":"Process Zillow Data\u00b6","text":"<p>The county-level Zillow ZHVI dataset contains a <code>StateCodeFIPS</code> and <code>MunicipalCodeFIPS</code> column that corresponds to the state and county FIPS codes. We need to format the FIPS codes to match the PDFM embeddings' <code>postal_code</code> format, which looks like <code>geoId/XXYYY</code>.</p>"},{"location":"workshops/AIforGood_2025/#request-access-to-pdfm-embeddings","title":"Request access to PDFM Embeddings\u00b6","text":"<p>The PDFM embeddings zip file you downloaded earlier contains a <code>county.geojson</code> file, which contains US county boundaries.</p>"},{"location":"workshops/AIforGood_2025/#load-county-boundaries","title":"Load county boundaries\u00b6","text":""},{"location":"workshops/AIforGood_2025/#join-home-value-data-and-county-boundaries","title":"Join home value data and county boundaries\u00b6","text":""},{"location":"workshops/AIforGood_2025/#visualize-home-values-in-2d","title":"Visualize home values in 2D\u00b6","text":""},{"location":"workshops/AIforGood_2025/#visualize-home-values-in-3d","title":"Visualize home values in 3D\u00b6","text":""},{"location":"workshops/AIforGood_2025/#load-pdfm-county-embeddings","title":"Load PDFM county embeddings\u00b6","text":""},{"location":"workshops/AIforGood_2025/#visualize-pdfm-features","title":"Visualize PDFM features\u00b6","text":"<p>Select any of the 330 PDFM features to visualize.</p>"},{"location":"workshops/AIforGood_2025/#join-zillow-and-pdfm-data","title":"Join Zillow and PDFM Data\u00b6","text":""},{"location":"workshops/AIforGood_2025/#split-train-and-test-data","title":"Split Train and Test Data\u00b6","text":""},{"location":"workshops/AIforGood_2025/#fit-linear-regression-model","title":"Fit Linear Regression Model\u00b6","text":""},{"location":"workshops/AIforGood_2025/#evaluate-linear-regression-model","title":"Evaluate Linear Regression Model\u00b6","text":""},{"location":"workshops/AIforGood_2025/#join-predicted-values-with-county-boundaries","title":"Join predicted values with county boundaries\u00b6","text":""},{"location":"workshops/AIforGood_2025/#visualize-actual-home-values","title":"Visualize actual home values\u00b6","text":""},{"location":"workshops/AIforGood_2025/#visualize-predicted-home-values","title":"Visualize predicted home values\u00b6","text":""},{"location":"workshops/AIforGood_2025/#visualize-difference-between-predicted-and-actual-home-values","title":"Visualize difference between predicted and actual home values\u00b6","text":""}]}